# SALES DATA CLUSTERING USING K-MEANS AND HIERARCHICAL CLUSTERING
# Dataset: https://www.kaggle.com/datasets/kyanyoga/sample-sales-data

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster

# 1. Load dataset
df = pd.read_csv("sales_data_sample.csv", encoding='latin1')
print("Dataset preview:")
print(df.head())

# 2. Preprocessing
# Select numeric columns only for clustering
numeric_df = df.select_dtypes(include=[np.number]).dropna(axis=1, how='all')

print("\nNumeric Columns Used:")
print(numeric_df.columns.tolist())

# Scale numeric data for better clustering
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_df)

# 3. Determine optimal number of clusters using Elbow Method
inertia = []
K = range(1, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)

# Plot elbow curve
plt.figure(figsize=(8, 5))
plt.plot(K, inertia, 'bo--')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (Within-Cluster Sum of Squares)')
plt.grid(True)
plt.show()

# Based on the elbow plot, choose k (for example 3 or 4)
optimal_k = 4  # Change based on elbow visualization

# 4. Apply K-Means with chosen number of clusters
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df['Cluster'] = kmeans.fit_predict(scaled_data)

# 5. Visualize cluster distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='Cluster', data=df, palette='viridis')
plt.title('Cluster Distribution')
plt.show()

# 6. Optional: visualize relationship between two features
if 'SALES' in numeric_df.columns and 'QUANTITYORDERED' in numeric_df.columns:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(
        x=df['SALES'], y=df['QUANTITYORDERED'], hue=df['Cluster'], palette='Set2'
    )
    plt.title('Sales vs Quantity by Cluster')
    plt.show()

# 7. Hierarchical Clustering (optional visualization)
linked = linkage(scaled_data, method='ward')
plt.figure(figsize=(10, 6))
dendrogram(linked, truncate_mode='lastp', p=10, show_leaf_counts=True)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Cluster Size')
plt.ylabel('Distance')
plt.show()

print("\nâœ… Clustering complete! Cluster labels added as 'Cluster' column.")
print(df[['Cluster']].head())
